{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model to Predict Change in Sentiments\n",
    "\n",
    "Here, I use an LSTM model to predict the change in the sentiment of an incoming utterance given the previous sentiment. This model takes as inputs word sequences and the sentiment of the previous utterance. This model is able to take word order into account. Similarly with the previous models, I used a pre-trained word embedding to represent words in 50-dimensional GloVe embeddings. For my implementation, I used Keras with Tensorflow as the backend. \n",
    "\n",
    "## Mini-batch Training\n",
    "Messages have different lengths and all the input sequences must have the same length to train the model with mini-batches. According to DailyDialog: A Manually Labelled Multi-turn Dialogue Dataset (https://arxiv.org/pdf/1710.03957.pdf), the average number of tokens per utterance is 14.9. Therefore, I set the maximum length of the incoming sequences to 15 tokens. Any messages shorter than 15 were padded with zeros and the ones longer than 15 were right truncated. The batch size is 32 and I trained the model for 10 epochs.\n",
    "\n",
    "## Overview of the Model\n",
    "Here is the sentiment change prediction model that I used\n",
    "![alt text](lstm_model.jpg \"Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zahra/anaconda3/envs/tensorflow/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/Users/zahra/anaconda3/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from utility import *\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation, Concatenate\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50-dimensional GloVe embeddings\n",
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('glove.6B/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indices(samples, word_to_index, max_len):\n",
    "    '''\n",
    "    this function converts an array of sentences (strings) into an array of indices corresponding \n",
    "    to words in the sentences.\n",
    "    The output shape should be such that it can be given to Embedding(). \n",
    "    '''\n",
    "    m = len(samples)                                   # number of training examples\n",
    "    X_indices = np.zeros((m, max_len))\n",
    "    \n",
    "    for i, sentence in enumerate(samples):            # loop over training examples\n",
    "        \n",
    "        sentence = re.sub(r'[^\\w\\s]', ' ', sentence.strip())  \n",
    "        words = [i.lower() for i in sentence.strip().split()]\n",
    "        j = 0\n",
    "        \n",
    "        for w in words:\n",
    "            if w in word_to_index:\n",
    "                X_indices[i, j] = word_to_index[w]\n",
    "            else:\n",
    "                X_indices[i, j] = word_to_index['unk']\n",
    "            j += 1\n",
    "            if j == max_len: break\n",
    "                \n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(samples_list):\n",
    "    '''\n",
    "    this function creates the inputs and output of the model\n",
    "    '''\n",
    "    X, Y, aux_X = [], [], []\n",
    "    for sample in samples_list:            \n",
    "        X.append(sample['utterance'])\n",
    "        Y.append(sample['current_emotion'] - sample['prev_emotion'])\n",
    "        aux_X.append(sample['prev_emotion'])\n",
    "    \n",
    "    return X, aux_X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    '''\n",
    "    this function creates an embedding layer using glove.6B\n",
    "    '''\n",
    "    vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding (requirement)\n",
    "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      # define dimensionality of your GloVe word vectors (= 50)\n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    \n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = word_to_vec_map[word]\n",
    "\n",
    "    embedding_layer = Embedding(vocab_len, emb_dim, trainable=False)\n",
    "    embedding_layer.build((None,))\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def coeff_determination(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square( y_true-y_pred )) \n",
    "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) ) \n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_model(input_shape, word_to_vec_map, word_to_index):\n",
    "    '''\n",
    "    this function creates the rnn model\n",
    "    '''\n",
    "    sentence_indices = Input(input_shape, dtype='int32')\n",
    "    aux_input = Input((1, ), dtype='float32')\n",
    "    \n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    embeddings = embedding_layer(sentence_indices)   \n",
    "    \n",
    "    X = LSTM(128, return_sequences=True)(embeddings)\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = LSTM(128, return_sequences=False)(X)\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = Dense(1)(X)\n",
    "    \n",
    "    merged = Concatenate()([aux_input, X])\n",
    "    merged = Dense(1)(merged)\n",
    "    output = Activation('tanh')(merged)\n",
    "    model = Model(inputs=[aux_input, sentence_indices], outputs=output)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train, test and validation sets\n",
    "train_conversations = load_conversations(category='train')\n",
    "find_sentiments(train_conversations)\n",
    "\n",
    "test_conversations = load_conversations(category='test')\n",
    "find_sentiments(test_conversations)\n",
    "\n",
    "validation_conversations = load_conversations(category='validation')\n",
    "find_sentiments(validation_conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# any messages longer than max length will be cut \n",
    "# and messages shorter than max length will be padded with 0\n",
    "maxLen = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples:  76052 76052 76052\n"
     ]
    }
   ],
   "source": [
    "# create training samples\n",
    "train_samples = create_samples(train_conversations)\n",
    "x_train, aux_x_train, y_train = create_features(train_samples)\n",
    "print(\"Number of training samples: \", len(x_train), len(aux_x_train), len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.2   ],\n",
       "       [-0.35  ],\n",
       "       [-0.3125],\n",
       "       ...,\n",
       "       [ 0.    ],\n",
       "       [ 0.    ],\n",
       "       [ 0.    ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change the shape of auxilary input: previous emotion\n",
    "aux_x_train = np.array(aux_x_train) \n",
    "aux_x_train.reshape(-1, 1)\n",
    "# print(aux_x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 15)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 15, 50)       20000050    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 15, 128)      91648       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 15, 128)      0           lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 128)          131584      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128)          0           lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            129         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 2)            0           input_2[0][0]                    \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            3           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1)            0           dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 20,223,414\n",
      "Trainable params: 223,364\n",
      "Non-trainable params: 20,000,050\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create the sentiment model and show the model summary\n",
    "model = sentiment_model((maxLen,), word_to_vec_map, word_to_index)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "76052/76052 [==============================] - 146s 2ms/step - loss: 0.0143 - mean_squared_error: 0.0143 - coeff_determination: 0.8902\n",
      "Epoch 2/10\n",
      "76052/76052 [==============================] - 149s 2ms/step - loss: 0.0134 - mean_squared_error: 0.0134 - coeff_determination: 0.8976\n",
      "Epoch 3/10\n",
      "76052/76052 [==============================] - 155s 2ms/step - loss: 0.0127 - mean_squared_error: 0.0127 - coeff_determination: 0.9021\n",
      "Epoch 4/10\n",
      "76052/76052 [==============================] - 157s 2ms/step - loss: 0.0121 - mean_squared_error: 0.0121 - coeff_determination: 0.9076\n",
      "Epoch 5/10\n",
      "76052/76052 [==============================] - 159s 2ms/step - loss: 0.0116 - mean_squared_error: 0.0116 - coeff_determination: 0.9120\n",
      "Epoch 6/10\n",
      "76052/76052 [==============================] - 162s 2ms/step - loss: 0.0110 - mean_squared_error: 0.0110 - coeff_determination: 0.9160\n",
      "Epoch 7/10\n",
      "76052/76052 [==============================] - 165s 2ms/step - loss: 0.0106 - mean_squared_error: 0.0106 - coeff_determination: 0.9194\n",
      "Epoch 8/10\n",
      "76052/76052 [==============================] - 175s 2ms/step - loss: 0.0102 - mean_squared_error: 0.0102 - coeff_determination: 0.9230\n",
      "Epoch 9/10\n",
      "76052/76052 [==============================] - 179s 2ms/step - loss: 0.0097 - mean_squared_error: 0.0097 - coeff_determination: 0.9265\n",
      "Epoch 10/10\n",
      "76052/76052 [==============================] - 173s 2ms/step - loss: 0.0094 - mean_squared_error: 0.0094 - coeff_determination: 0.9292\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a2f77f4a8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile and fit the model\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mse', coeff_determination])\n",
    "x_train_indices = sentences_to_indices(x_train, word_to_index, maxLen)\n",
    "model.fit([aux_x_train, x_train_indices], y_train, epochs=10, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6740/6740 [==============================] - 3s 462us/step\n",
      "Test mean square error =  0.017323209428800498\n",
      "Test mean square error =  0.8795359571538977\n"
     ]
    }
   ],
   "source": [
    "# create the test samples\n",
    "test_samples = create_samples(test_conversations)\n",
    "x_test, aux_x_test, y_test = create_features(test_samples)\n",
    "\n",
    "# reshape the auxilary test input\n",
    "aux_x_test = np.array(aux_x_test) \n",
    "aux_x_test.reshape(-1, 1)\n",
    "\n",
    "# evaluate the model with the test set\n",
    "x_test_indices = sentences_to_indices(x_test, word_to_index, max_len=maxLen)\n",
    "loss, mse, r2 = model.evaluate([aux_x_test, x_test_indices], y_test)\n",
    "\n",
    "print(\"Test mean square error = \", mse)\n",
    "print(\"Test r-squared = \", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7069/7069 [==============================] - 3s 464us/step\n",
      "Validation mean square error =  0.017323209428800498\n",
      "Validation r-squared:  0.8747538389554539\n"
     ]
    }
   ],
   "source": [
    "# create the test samples\n",
    "validation_samples = create_samples(validation_conversations)\n",
    "x_validation, aux_x_validation, y_validation = create_features(validation_samples)\n",
    "\n",
    "# reshape the auxilary test input\n",
    "aux_x_validation = np.array(aux_x_validation) \n",
    "aux_x_validation.reshape(-1, 1)\n",
    "\n",
    "# evaluate the model with the test set\n",
    "x_validation_indices = sentences_to_indices(x_validation, word_to_index, max_len=maxLen)\n",
    "loss, ms, r2 = model.evaluate([aux_x_validation, x_validation_indices], y_validation)\n",
    "\n",
    "print(\"Validation mean square error = \", mse)\n",
    "print(\"Validation r-squared: \", r2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
